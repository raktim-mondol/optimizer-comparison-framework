\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\title{AMCAS Optimizer: Mathematical Formulation and Convergence Proof}
\author{}
\date{}

\begin{document}

\maketitle

\section{AMCAS Algorithm}

The Adaptive Momentum with Curvature-Aware Scaling (AMCAS) optimizer updates parameters as follows:

\subsection{Core Update Equations}

\begin{align}
    &\text{Adaptive Momentum:} \quad \beta_{1,t} = \beta_1 \exp\left(-\lambda \|\bm{g}_t - \bm{g}_{t-1}\|^2\right) \\
    &\bm{m}_t = \beta_{1,t} \bm{m}_{t-1} + (1 - \beta_{1,t}) \bm{g}_t \\
    &\text{Second Moment:} \quad \bm{v}_t = \beta_2 \bm{v}_{t-1} + (1 - \beta_2) (\bm{g}_t \odot \bm{g}_t) \\
    &\text{Curvature Estimate:} \quad h_{t,i} = (1 - \gamma) h_{t-1,i} + \gamma \frac{g_{t,i}^2}{v_{t-1,i} + \epsilon} \\
    &\text{Parameter Update:} \quad \bm{\theta}_{t+1} = \bm{\theta}_t - \alpha_t \frac{\bm{m}_t}{\sqrt{\bm{v}_t} + \epsilon}
\end{align}

\subsection{Trust Region Adaptation}

\begin{align}
    &\Delta_{\text{pred}} = -\alpha_t \bm{m}_t^\top \bm{g}_t + \frac{1}{2} \alpha_t^2 \bm{m}_t^\top \bm{H}_t \bm{m}_t \\
    &\Delta_{\text{actual}} = L(\bm{\theta}_t) - L(\bm{\theta}_t + \alpha_t \bm{m}_t) \\
    &\alpha_{t+1} = \begin{cases}
        \alpha_t \cdot \tau_{\text{increase}} & \text{if } \frac{\Delta_{\text{actual}}}{\Delta_{\text{pred}}} > \eta_{\text{high}} \\
        \alpha_t \cdot \tau_{\text{decrease}} & \text{if } \frac{\Delta_{\text{actual}}}{\Delta_{\text{pred}}} < \eta_{\text{low}} \\
        \alpha_t & \text{otherwise}
    \end{cases}
\end{align}

\section{Key Lemmas}

\subsection{Lemma 1: Momentum Bound}
Under bounded gradients $\|\bm{g}_t\| \leq G$, we have:
\[
\|\bm{m}_t\| \leq \frac{1 - \beta_1^t}{1 - \beta_1} G.
\]

\subsection{Lemma 2: Curvature Bounds}
The curvature estimates satisfy:
\[
\mu \leq h_{t,i} \leq M \quad \text{for all } t, i
\]
where $\mu = \min(1, \gamma\epsilon/G^2)$ and $M = \max(1, G^2/\epsilon)$.

\subsection{Lemma 3: Trust Region Guarantee}
For sufficiently small step sizes:
\[
\Delta_{\text{actual}} \geq \eta_{\text{low}} \Delta_{\text{pred}}.
\]

\section{Convergence Theorem}

\subsection{Theorem: AMCAS Convergence}
Under standard assumptions (smoothness, bounded gradients, bounded variance, bounded curvature, and trust region parameters), AMCAS with $\alpha_t = \alpha/\sqrt{t}$ achieves:
\[
\frac{1}{T} \sum_{t=1}^T \mathbb{E}[\|\nabla L(\bm{\theta}_t)\|^2] \leq \frac{C}{\sqrt{T}}
\]
where $C$ depends on problem constants.

\subsection{Proof Sketch}

\begin{enumerate}
    \item \textbf{Smoothness}: $L(\bm{\theta}_{t+1}) \leq L(\bm{\theta}_t) - \alpha_t \nabla L(\bm{\theta}_t)^\top \bm{d}_t + \frac{L\alpha_t^2}{2} \|\bm{d}_t\|^2$
    
    \item \textbf{Momentum Analysis}: $\nabla L(\bm{\theta}_t)^\top \bm{d}_t \leq -\frac{c}{\sqrt{\mu}+\epsilon} \|\nabla L(\bm{\theta}_t)\|^2 + \text{error}$
    
    \item \textbf{Telescoping Sum}: Sum over $t=1$ to $T$ and rearrange
    
    \item \textbf{Rate}: With $\alpha_t = \alpha/\sqrt{t}$, obtain $O(1/\sqrt{T})$ rate
\end{enumerate}

\section{Advantages Over Adam}

\begin{itemize}
    \item \textbf{Adaptive Momentum}: $\beta_{1,t}$ decreases for noisy gradients, preventing overshooting
    \item \textbf{Curvature Scaling}: Diagonal Hessian approximation provides second-order information
    \item \textbf{Trust Region}: Dynamic step size adjustment prevents divergence
    \item \textbf{Theoretical Guarantees}: Provable convergence under standard assumptions
\end{itemize}

\end{document}