# MNIST CNN Experiment Configuration
# Compares different optimizers on MNIST dataset with CNN architecture

experiment_name: "mnist_cnn_comparison"
dataset: "mnist"
model: "simple_cnn"
epochs: 10
batch_size: 64
learning_rate: 0.001
data_augmentation: false
use_scheduler: false
seed: 42

optimizers:
  - name: "AMCAS"
    class: "AMCAS"
    params:
      betas: [0.9, 0.999]
      gamma: 0.1
      lambda_consistency: 0.01
      
  - name: "Adam"
    class: "Adam"
    params:
      betas: [0.9, 0.999]
      
  - name: "AdamW"
    class: "AdamW"
    params:
      betas: [0.9, 0.999]
      weight_decay: 0.01
      
  - name: "SGD"
    class: "SGD"
    params: {}
    
  - name: "SGD+Momentum"
    class: "SGD+Momentum"
    params: {}  # momentum is handled in the lambda function
      
  - name: "RMSprop"
    class: "RMSprop"
    params: {}
    
  - name: "Adagrad"
    class: "Adagrad"
    params: {}
    
  - name: "Adadelta"
    class: "Adadelta"
    params: {}
    
  - name: "NAdam"
    class: "NAdam"
    params:
      betas: [0.9, 0.999]
      
  - name: "RAdam"
    class: "RAdam"
    params:
      betas: [0.9, 0.999]

metrics:
  classification:
    - accuracy
    - precision
    - recall
    - f1
    - auc_roc
    
  computational:
    - training_time
    - inference_time
    - memory_usage
    - flops
    
  optimizer_specific:
    - gradient_consistency
    - curvature_stats
    - trust_ratio

output:
  excel_file: "results/mnist_cnn_comparison.xlsx"
  plots_dir: "results/plots/mnist_cnn"
  json_file: "results/raw/mnist_cnn.json"
  report_file: "results/reports/mnist_cnn_report.md"