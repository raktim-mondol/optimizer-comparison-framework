# MNIST Vision Transformer Experiment Configuration
# Compares different optimizers on MNIST dataset with Vision Transformer architecture

experiment_name: "mnist_vit_comparison"
dataset: "mnist"
model: "vit_small"
epochs: 15
batch_size: 32
learning_rate: 0.0001
data_augmentation: true
use_scheduler: true
seed: 42

optimizers:
  - name: "AMCAS"
    class: "AMCAS"
    params:
      betas: [0.9, 0.999]
      gamma: 0.05
      lambda_consistency: 0.005
      
  - name: "Adam"
    class: "Adam"
    params:
      betas: [0.9, 0.999]
      
  - name: "AdamW"
    class: "AdamW"
    params:
      betas: [0.9, 0.999]
      weight_decay: 0.01
      
  - name: "SGD"
    class: "SGD"
    params:
      momentum: 0.9
      
  - name: "RMSprop"
    class: "RMSprop"
    params: {}
    
  - name: "NAdam"
    class: "NAdam"
    params:
      betas: [0.9, 0.999]
      
  - name: "RAdam"
    class: "RAdam"
    params:
      betas: [0.9, 0.999]

metrics:
  classification:
    - accuracy
    - precision
    - recall
    - f1
    - auc_roc
    
  computational:
    - training_time
    - inference_time
    - memory_usage
    - flops
    
  optimizer_specific:
    - gradient_consistency
    - curvature_stats
    - trust_ratio

output:
  excel_file: "results/mnist_vit_comparison.xlsx"
  plots_dir: "results/plots/mnist_vit"
  json_file: "results/raw/mnist_vit.json"
  report_file: "results/reports/mnist_vit_report.md"